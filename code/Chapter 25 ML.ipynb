{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hostname\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/01 21:51:54 WARN Utils: Your hostname, DESKTOP-CHAV9A4 resolves to a loopback address: 127.0.1.1; using 172.20.114.226 instead (on interface eth0)\n",
      "22/07/01 21:51:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/01 21:51:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(\"hostname\", \"172.20.116.69\").\\\n",
    "    appName(\"github_csv\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"../data/retail-data/by-day/*.csv\")\n",
    "sales.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales = sales.coalesce(5)\\\n",
    "  .where(\"Description IS NOT NULL\")\n",
    "sales.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|int1|int2|int3|\n",
      "+----+----+----+\n",
      "|   4|   5|   6|\n",
      "|   7|   8|   9|\n",
      "|   1|   2|   3|\n",
      "+----+----+----+\n",
      "\n",
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|green|good|     1|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "+---+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fakeIntDF = spark.read.parquet(\"../data/simple-ml-integers\")\n",
    "fakeIntDF.show(3)\n",
    "simpleDF = spark.read.json(\"../data/simple-ml\")\n",
    "simpleDF.show(3)\n",
    "scaleDF = spark.read.parquet(\"../data/simple-ml-scaling\")\n",
    "scaleDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|color| lab|value1|            value2|            features|label|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "supervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")\n",
    "supervised.fit(simpleDF).transform(simpleDF).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+\n",
      "|sum(Quantity)|count(1)|CustomerID|\n",
      "+-------------+--------+----------+\n",
      "|          119|      62|   14452.0|\n",
      "|          440|     143|   16916.0|\n",
      "|          630|      72|   17633.0|\n",
      "|           34|       6|   14768.0|\n",
      "|         1542|      30|   13094.0|\n",
      "|          854|     117|   17884.0|\n",
      "|           97|      12|   16596.0|\n",
      "|          290|      98|   13607.0|\n",
      "+-------------+--------+----------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "basicTransformation = SQLTransformer()\\\n",
    "  .setStatement(\"\"\"\n",
    "    SELECT sum(Quantity), count(*), CustomerID\n",
    "    FROM __THIS__\n",
    "    GROUP BY CustomerID\n",
    "  \"\"\")\n",
    "\n",
    "basicTransformation.transform(sales).show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------------------------------------+\n",
      "|int1|int2|int3|VectorAssembler_562655a118c7__output|\n",
      "+----+----+----+------------------------------------+\n",
      "|   4|   5|   6|                       [4.0,5.0,6.0]|\n",
      "|   7|   8|   9|                       [7.0,8.0,9.0]|\n",
      "|   1|   2|   3|                       [1.0,2.0,3.0]|\n",
      "+----+----+----+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler().setInputCols([\"int1\", \"int2\", \"int3\"])\n",
    "va.transform(fakeIntDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler().setInputCols([\"int1\", \"int2\", \"int3\"])\n",
    "va.transform(fakeIntDF).show()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "contDF = spark.range(20).selectExpr(\"cast(id as double)\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "bucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0]\n",
    "bucketer = Bucketizer().setSplits(bucketBorders).setInputCol(\"id\")\n",
    "bucketer.transform(contDF).show()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "bucketer = QuantileDiscretizer().setNumBuckets(5).setInputCol(\"id\")\n",
    "fittedBucketer = bucketer.fit(contDF)\n",
    "fittedBucketer.transform(contDF).show()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "sScaler = StandardScaler().setInputCol(\"features\")\n",
    "sScaler.fit(scaleDF).transform(scaleDF).show()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "minMax = MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\")\n",
    "fittedminMax = minMax.fit(scaleDF)\n",
    "fittedminMax.transform(scaleDF).show()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "maScaler = MaxAbsScaler().setInputCol(\"features\")\n",
    "fittedmaScaler = maScaler.fit(scaleDF)\n",
    "fittedmaScaler.transform(scaleDF).show()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "scaleUpVec = Vectors.dense(10.0, 15.0, 20.0)\n",
    "scalingUp = ElementwiseProduct()\\\n",
    "  .setScalingVec(scaleUpVec)\\\n",
    "  .setInputCol(\"features\")\n",
    "scalingUp.transform(scaleDF).show()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import Normalizer\n",
    "manhattanDistance = Normalizer().setP(1).setInputCol(\"features\")\n",
    "manhattanDistance.transform(scaleDF).show()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "lblIndxr = StringIndexer().setInputCol(\"lab\").setOutputCol(\"labelInd\")\n",
    "idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)\n",
    "idxRes.show()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "valIndexer = StringIndexer().setInputCol(\"value1\").setOutputCol(\"valueInd\")\n",
    "valIndexer.fit(simpleDF).transform(simpleDF).show()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import IndexToString\n",
    "labelReverse = IndexToString().setInputCol(\"labelInd\")\n",
    "labelReverse.transform(idxRes).show()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "idxIn = spark.createDataFrame([\n",
    "  (Vectors.dense(1, 2, 3),1),\n",
    "  (Vectors.dense(2, 5, 6),2),\n",
    "  (Vectors.dense(1, 8, 9),3)\n",
    "]).toDF(\"features\", \"label\")\n",
    "indxr = VectorIndexer()\\\n",
    "  .setInputCol(\"features\")\\\n",
    "  .setOutputCol(\"idxed\")\\\n",
    "  .setMaxCategories(2)\n",
    "indxr.fit(idxIn).transform(idxIn).show()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "lblIndxr = StringIndexer().setInputCol(\"color\").setOutputCol(\"colorInd\")\n",
    "colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select(\"color\"))\n",
    "ohe = OneHotEncoder().setInputCol(\"colorInd\")\n",
    "ohe.transform(colorLab).show()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn.transform(sales.select(\"Description\"))\n",
    "tokenized.show(20, False)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "rt = RegexTokenizer()\\\n",
    "  .setInputCol(\"Description\")\\\n",
    "  .setOutputCol(\"DescOut\")\\\n",
    "  .setPattern(\" \")\\\n",
    "  .setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(20, False)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "rt = RegexTokenizer()\\\n",
    "  .setInputCol(\"Description\")\\\n",
    "  .setOutputCol(\"DescOut\")\\\n",
    "  .setPattern(\" \")\\\n",
    "  .setGaps(False)\\\n",
    "  .setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(20, False)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover()\\\n",
    "  .setStopWords(englishStopWords)\\\n",
    "  .setInputCol(\"DescOut\")\n",
    "stops.transform(tokenized).show()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import NGram\n",
    "unigram = NGram().setInputCol(\"DescOut\").setN(1)\n",
    "bigram = NGram().setInputCol(\"DescOut\").setN(2)\n",
    "unigram.transform(tokenized.select(\"DescOut\")).show(False)\n",
    "bigram.transform(tokenized.select(\"DescOut\")).show(False)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer()\\\n",
    "  .setInputCol(\"DescOut\")\\\n",
    "  .setOutputCol(\"countVec\")\\\n",
    "  .setVocabSize(500)\\\n",
    "  .setMinTF(1)\\\n",
    "  .setMinDF(2)\n",
    "fittedCV = cv.fit(tokenized)\n",
    "fittedCV.transform(tokenized).show(False)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "tfIdfIn = tokenized\\\n",
    "  .where(\"array_contains(DescOut, 'red')\")\\\n",
    "  .select(\"DescOut\")\\\n",
    "  .limit(10)\n",
    "tfIdfIn.show(10, False)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "tf = HashingTF()\\\n",
    "  .setInputCol(\"DescOut\")\\\n",
    "  .setOutputCol(\"TFOut\")\\\n",
    "  .setNumFeatures(10000)\n",
    "idf = IDF()\\\n",
    "  .setInputCol(\"TFOut\")\\\n",
    "  .setOutputCol(\"IDFOut\")\\\n",
    "  .setMinDocFreq(2)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "idf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).show(10, False)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\",\n",
    "  outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA().setInputCol(\"features\").setK(2)\n",
    "pca.fit(scaleDF).transform(scaleDF).show(20, False)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "pe = PolynomialExpansion().setInputCol(\"features\").setDegree(2)\n",
    "pe.transform(scaleDF).show()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import ChiSqSelector, Tokenizer\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn\\\n",
    "  .transform(sales.select(\"Description\", \"CustomerId\"))\\\n",
    "  .where(\"CustomerId IS NOT NULL\")\n",
    "prechi = fittedCV.transform(tokenized)\\\n",
    "  .where(\"CustomerId IS NOT NULL\")\n",
    "chisq = ChiSqSelector()\\\n",
    "  .setFeaturesCol(\"countVec\")\\\n",
    "  .setLabelCol(\"CustomerId\")\\\n",
    "  .setNumTopFeatures(2)\n",
    "chisq.fit(prechi).transform(prechi)\\\n",
    "  .drop(\"customerId\", \"Description\", \"DescOut\").show()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "fittedPCA = pca.fit(scaleDF)\n",
    "fittedPCA.write().overwrite().save(\"/tmp/fittedPCA\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import PCAModel\n",
    "loadedPCA = PCAModel.load(\"/tmp/fittedPCA\")\n",
    "loadedPCA.transform(scaleDF).show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
